{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1e0979",
   "metadata": {},
   "source": [
    "# üß† cloudChronicles Lab #001: Disaster Recovery Detective\n",
    "\n",
    "**Lab Type:** Idea  \n",
    "**Estimated Time:** 30‚Äì45 mins  \n",
    "**Skill Level:** Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by printing your name to personalize the notebook\n",
    "your_name = \"\"\n",
    "print(f\"Welcome to the lab, {your_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94d019",
   "metadata": {},
   "source": [
    "## üîç STAR Method Lab Prompt\n",
    "\n",
    "**Situation:**  \n",
    "[A regional outage in Google Cloud's us-central1 region presents a significant challenge, but with a well-structured disaster recovery plan, we can minimize downtime and data loss. As a Cloud Architect, I've designed a STAR (Situation, Task, Action, Result) based plan leveraging Google Cloud's native high-availability and disaster recovery capabilities.

Disaster Recovery Plan: Google Cloud us-central1 Regional Outage
Situation:
On June 2nd, 2025, at 11:35 AM EDT, a major outage has been declared in Google Cloud's us-central1 region. This outage has impacted all Google Cloud services within this region, rendering our primary production environment inaccessible. Our business-critical applications and data, primarily hosted in us-central1, are currently unavailable, leading to a significant impact on our operations and customer accessibility. Our Recovery Time Objective (RTO) is 4 hours, and our Recovery Point Objective (RPO) is 15 minutes.

Task:
The immediate task is to execute a predefined disaster recovery plan to failover our critical services to a secondary, operational Google Cloud region, restore business continuity, and ensure data integrity. The ultimate goal is to achieve an RTO within 4 hours and an RPO within 15 minutes, minimizing the impact of the outage on our business and customers.

Action:
Our disaster recovery strategy is built upon a multi-region active-passive architecture for critical services, with robust data replication and automated failover mechanisms. We will be leveraging us-east4 (Northern Virginia) as our designated disaster recovery region.

Phase 1: Detection and Notification (Immediate)

Pub/Sub Alerts:
Action: Automated alerts are configured via Cloud Monitoring and Cloud Logging to trigger Pub/Sub topics upon detection of critical service health issues or regional unavailability in us-central1. These alerts are subscribed to by our on-call operations team and automated failover scripts.
Example Alerts:
google.cloud.sql.database.instance.status.DOWN
google.compute.zone.status.UNAVAILABLE
High error rates on load balancers pointing to us-central1.
Outcome: Rapid notification to the incident response team and initiation of the failover process.
Phase 2: Database Failover (Cloud SQL Replicas - RPO < 15 minutes)

Pre-configured Cross-Region Read Replicas:
Action: Our production Cloud SQL instances in us-central1 have asynchronous cross-region read replicas configured in us-east4. These replicas are continuously updated with changes from the primary instance.
Verification: Confirm the replication lag for all critical databases is within our RPO of 15 minutes.
Promote Read Replica to Primary:
Action: Upon confirmation of the us-central1 outage via Pub/Sub alerts and manual verification, the designated read replica in us-east4 will be promoted to a standalone primary instance. This is a manual or script-driven process to ensure controlled failover.
gcloud command: gcloud sql instances promote-replica [REPLICA_INSTANCE_NAME] --region=us-east4
Outcome: Database services are restored in us-east4 with minimal data loss, adhering to the RPO.
Phase 3: Application Failover (Compute Engine/GKE & Load Balancing - RTO < 4 hours)

Multi-Region Deployment Strategy:
Pre-configured Resources: We maintain pre-provisioned, scaled-down Compute Engine instances or GKE clusters in us-east4 that are ready to scale up. Our application images and configurations are stored in Artifact Registry (which is a global service).
Traffic Steering (Global External Load Balancer): Our Global External Load Balancer is configured with backends in both us-central1 (primary) and us-east4 (secondary).
Action:
Automatic Health Checks: The Global External Load Balancer's health checks will automatically detect the unhealthiness of the us-central1 backends.
Traffic Rerouting: Traffic will automatically be routed away from us-central1 and directed to the healthy us-east4 backends.
Scaling Up: Automated scripts or a CI/CD pipeline will be triggered to scale up the Compute Engine instances or GKE nodes in us-east4 to handle the full production load. This includes attaching persistent disks (if not already replicated or snapshotted).
DNS Update (if necessary): While the Global External Load Balancer handles traffic routing, if any direct DNS entries point to us-central1 resources (e.g., for internal services), these will be updated via Cloud DNS to point to us-east4 equivalents.
Outcome: Application services become accessible in us-east4, with traffic seamlessly redirected, contributing to the RTO.
Phase 4: Data Recovery and Synchronization (Multi-Region Cloud Storage)

Multi-Region Cloud Storage Buckets:
Action: All critical application data, user-generated content, and backups are stored in Multi-Region Cloud Storage buckets (e.g., us-east4 or nam4 for North America). This ensures data is automatically replicated across multiple regions within the chosen multi-region configuration, making it resilient to a single regional outage.
Access: Applications in us-east4 will seamlessly access the data from the Multi-Region Cloud Storage bucket.
Outcome: Data availability and integrity are maintained throughout the outage.
Phase 5: Post-Failover Verification and Monitoring

Comprehensive Health Checks:
Action: Once failover is complete, a series of automated and manual health checks will be performed across all services in us-east4. This includes database connectivity, application functionality, API endpoints, and user experience.
Monitoring: Cloud Monitoring dashboards and custom alerts will be actively monitored to ensure the stability and performance of the services in the failover region.
Outcome: Confirmation of successful failover and stable operations in the disaster recovery region.
Phase 6: Reversion (When us-central1 is Restored)

Monitor us-central1 Restoration:
Action: Continuously monitor Google Cloud's status page and internal alerts for the full restoration of services in us-central1.
Database Synchronization:
Action: Once us-central1 is stable, the database in us-east4 will be configured as the primary, and a new read replica will be created in us-central1. Once synchronized, the database in us-east4 will be demoted and the us-central1 instance promoted back to primary. This is a carefully planned maintenance window.
Application Re-routing:
Action: Gradually re-route traffic back to us-central1 via the Global External Load Balancer, either by re-enabling the us-central1 backends or by shifting weights if using advanced traffic management. Scale down resources in us-east4 once traffic is fully shifted.
Data Synchronization (if necessary for edge cases):
Action: For any ephemeral data not stored in Multi-Region Cloud Storage, a synchronization process will be initiated to bring us-central1 up to date.
Post-Reversion Verification:
Action: Verify the health and performance of all services in us-central1 and ensure a smooth transition back to the primary region.
Outcome: Full restoration of services in us-central1 and return to normal operations.
Result:
By executing this STAR-based disaster recovery plan, we anticipate the following results:

Recovery Time Objective (RTO) Achieved: Critical services will be restored and accessible within 2 hours, well within our 4-hour RTO. This is primarily achieved through automated traffic rerouting by the Global External Load Balancer and the rapid promotion of Cloud SQL replicas.
Recovery Point Objective (RPO) Achieved: Data loss will be minimized to less than 5 minutes, significantly better than our 15-minute RPO. This is due to the continuous, asynchronous replication of Cloud SQL read replicas and the inherent multi-region redundancy of Cloud Storage.
Minimal Business Impact: The rapid failover and restoration of services will significantly reduce the impact of the outage on our customers and internal operations, preserving revenue and customer satisfaction.
Enhanced Resilience: This incident will serve as a valuable test of our disaster recovery capabilities, allowing us to identify any areas for further improvement and strengthen our overall cloud infrastructure resilience.
Automated and Controlled Failover: Leveraging Google Cloud's native tools like Pub/Sub for alerts, Cloud SQL replicas for databases, and Global External Load Balancers for traffic management ensures a highly automated and controlled failover process, reducing manual errors and accelerating recovery.]\n",
    "\n",
    "**Task:**  \n",
    "[Define what the user is expected to solve.]\n",
    "\n",
    "**Action:**  \n",
    "[Step-by-step instructions using GCP tools.]\n",
    "\n",
    "**Expected Result:**  \n",
    "[A defined deliverable such as a DR plan, diagram, MVP, etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b221d",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Your Assignment\n",
    "\n",
    "_Use this section to complete your deliverable:_\n",
    "\n",
    "```markdown\n",
    "(Example Format)\n",
    "\n",
    "- **Primary Region**: us-central1  \n",
    "- **Backup Location**: us-east1  \n",
    "- **Failover Trigger**: Load balancer health check + Pub/Sub alert  \n",
    "- **Redundancy Services**:  \n",
    "   - Cloud SQL with failover  \n",
    "   - Cloud Storage versioning  \n",
    "   - Cloud Functions for health monitoring  \n",
    "- **Backup Schedule**: Every 6 hours, daily export to multi-region bucket  \n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
